{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 \n",
    "# Plotting Politics By Proxy\n",
    "<b>ASSIGNED: Friday, 29 Mar 2019.</b>   \n",
    "<b>DUE: Wednesday, 9 April 2019 at 5 pm</b>\n",
    "\n",
    "All students on both tracks must complete this homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "Using unsupervised learning we're going to produce a plot of US politicians via their individual language usage on Twitter, starting with ingesting the tweets, preparing these tweets for PCA, and finally performing PCA. We're going to use this exercise to reflect, analyze, and explore how such practices both reify and challenge our contemporary understanding of community and the public discourse. \n",
    "\n",
    "More specifically, you're going to\n",
    "1. tokenize and get word counts for a set of tweets;\n",
    "2. produce a PCA plot of twitter user accounts based on word usage;\n",
    "3. explore how word usage is (or isn't) a good proxy for political affliation for people in public office.\n",
    "\n",
    "#### To do this homework, you will make use of the following files:\n",
    "1. 10K2016tweets.csv\n",
    "2. pol_aff.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "This assignment is to be done <b>on your own</b>, but you can talk about the assignment with your classmates if you get stuck. (Be sure to list the students you spoke with about this assignment in the space provided below.) Feel free to also use [stackoverflow](https://stackoverflow.com/) but please provide citation and link to the specific answer if you do this. You may also visit Will Yumou during his TA office hours or email him with questions.\n",
    "\n",
    "Provide your code to justify your answer to each question. Your code must run with the \"10K2016tweets.csv\" and \"pol_aff.csv\" files as originally provided to you. \n",
    "\n",
    "Be sure to rename this homework notebook so that it includes your name. \n",
    "\n",
    "### List any students you talked with about this assignment here:\n",
    "1. [person 1]\n",
    "2. [person 2]\n",
    "3. etc.\n",
    "\n",
    "\n",
    "# Homework Problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Getting Word Frequencies For All Words in each tweet\n",
    "\n",
    "### question 1 [5 points]\n",
    "1. Ingest the file 10K2016tweets.csv into pandas as a dataframe entitled `tweets`. Note that each tweet is a row. We've provided the libraries you'll need for this assignment.\n",
    "2. Use code provdied to drop rows with NAN values.\n",
    "3. Use `tweets.tail()` to get a sense of what the data frame is. \n",
    "\n",
    "We've given you some libraries you'll need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code to read tweet csv into a dataframe called \"tweets\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run this code to drop rows with NAN values\n",
    "tweets.dropna(inplace=True)\n",
    "tweets = tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use tail() to see last five rows of dataframe \"tweets\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 2 [10 points]\n",
    "1. What day and time was the oldest tweet in the data set posted? Hint: use `tweets['created_at'].min()`. Also, be sure to make use of the `to_datetime()` command. \n",
    "2. What day and time was the most recent tweet in the data set posted?\n",
    "3. How many *tweets* does the data set have? \n",
    "4. How many unique twitter users does the data set have? Hint: use the `nunique()` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 3 [30 points]\n",
    "Use *sklearn's CountVectorizer* to produce a table of word counts in which each tweet is a row and each word is a column <b>for the first 100 tweets in your dataset</b>. Name this table \"wc\".\n",
    "\n",
    "#### For an EXAMPLE of using sklearn countvectorizer to produce this `wc`, see the code in the following 5 cells. Note that this example just uses three made-up tweets so that it's easier to see what's happening.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 4, 'animal': 0, 'is': 2, 'large': 3, 'can': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Here we list a few sample strings, where each \n",
    "# string can be considered one text. For the tweets \n",
    "# dataframe you'll to produce a list in which each tweet is one string.  \n",
    "# The string \"The animal is large\" is tweet 0 below; the string\n",
    "# \"is is is\" is tweet 1 below; etc.\n",
    "three_tweets = [\"The animal is large.\", \"is is is\", \"The can't\"]\n",
    "\n",
    "# create vectorizer \n",
    "cv = CountVectorizer()\n",
    "\n",
    "# tokenize texts & get vocabulary\n",
    "cv.fit(three_tweets) #note that the sklearn tokenizer turns \"can't\" into \"can\"\n",
    "\n",
    "# create a dictionary where each word is given an index number.\n",
    "# If you need to know what word a number represents, refer back to this dictionary\n",
    "word_index = cv.vocabulary_\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to find out what column number a particular word is, say, \"animal\" we can use \n",
    "word_index[\"animal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a table of tweets vs words\n",
    "word_counts = cv.transform(three_tweets)\n",
    "\n",
    "# and get the \"shape\" of this table like this:\n",
    "word_counts.shape\n",
    "# where the 3 rows are the three tweets and the \n",
    "# 5 columns are the 5 words in this corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animal</th>\n",
       "      <th>can</th>\n",
       "      <th>is</th>\n",
       "      <th>large</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   animal  can   is  large  the\n",
       "0     1.0  0.0  1.0    1.0  1.0\n",
       "1     0.0  0.0  3.0    0.0  0.0\n",
       "2     0.0  1.0  0.0    0.0  1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get dataframe of with words instead of word index numbers:\n",
    "feature_names = cv.get_feature_names()\n",
    "word_counts_df = pd.SparseDataFrame(word_counts, columns=feature_names).fillna(0)\n",
    "word_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1],\n",
       "       [0, 0, 3, 0, 0],\n",
       "       [0, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, you can dump the word_counts table into an array\n",
    "# where each row is a tweet, each column is a word type, and each\n",
    "# element is a word token count (where each word is denoted by a number\n",
    "# as defined in the word_index above). For the difference between word types and word tokens, \n",
    "# see \"type-token distinction\" in wikipedia.\n",
    "wc = word_counts.toarray()\n",
    "wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This concludes the example code.</b> \n",
    "\n",
    "\n",
    "Now do the same thing we did for the example above, but substitute the \"three_texts\" list in the above example with the first 100 tweets that appear in your data set.\n",
    "\n",
    "You can get a list called \"all_tweets\" of all your tweet texts using the following code: `all_tweets = tweets['tweet_text']`. To get the first 100 tweets, you can use a python slice: `all_tweets[0:100]`. We've provided this code to get you started.\n",
    "\n",
    "<b>At the very end of your code for this question, you should output `wc` just like we did in the example above.</b> \n",
    "\n",
    "#### Put your code for question 3 below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all_tweets from df as a list of strings. \n",
    "all_tweets = tweets['tweet_text']\n",
    "hundred_tweets = all_tweets[0:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Find word usage for each twitter account\n",
    "\n",
    "### question 4 [30 points]\n",
    "The table wc give us the word counts for all tweets, <b>but what we really want is a table of word counts for each twitter account in our first 100 tweets</b>.\n",
    "\n",
    "To do this: \n",
    "<b>Make a new array called `wc_accounts` where each row is a twitter account user and the columns are word token counts for unique word types.</b> Note that the dataframe \"tweets\" has a `user_id` provided for each tweet. \n",
    "\n",
    "To make `wc_accounts` use a FOR loop which runs over all `user_id`s. For each loop in the FOR loop you should do the following:\n",
    "1. get all the rows of one user's tweets from `tweets` in a list and convert this to a numpy array\n",
    "2. put this array of tweet rows into the `wc` array you built in question 3 (which will return all the words of user tweets), and use the `sum()` command on this resultant array to produce a single row with total word counts for one user\n",
    "3. take this single row of total word counts for one user and add this row `temp_results;\n",
    "\n",
    "Once the for loop is completed, you can save `temp_results` as `wc_accounts`.\n",
    "\n",
    "To get you started, here's a way to get a list of all `user_id`s, which you need to run your FOR loop: `twitter_users = tweets['user_id'].unique()`. We've provided this code.\n",
    "\n",
    "We've also provided you with some additional code. Your job is to fill in the appropriate commands in the FOR loop provided.\n",
    "\n",
    "Once you're done, be sure to output `wc_accounts`. (It will resemble the output for `wc`, but with different numbers.) We've provided this code for you too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Put your code for question 4 below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get twitter users from the dataframe\n",
    "twitter_users = tweets['user_id'].unique() # this gets all twitter user IDs\n",
    "temp_results = []                          # an empty list\n",
    "\n",
    "for user in twitter_users:\n",
    "    # insert your code for FOR loop here\n",
    "    # ...\n",
    "    # ...\n",
    "    # ...\n",
    "    # ...\n",
    "    # ...\n",
    "    \n",
    "wc_accounts = np.array(temp_results)\n",
    "wc_accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform PCA on wc_accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have wc_acounts, we'd like to plot this to see how all these accounts use language. But how plot this in two dimensions? Let's use a form of unsupervised learning called PCA, where each account (i.e., each row) represents one data point. This is the same as the PCA we performed on the Iris and Spearman data sets in our labs. We can plot our twitter account data in exactly in the same way we did then.   \n",
    "\n",
    "However, we have one additional wrinkle in our task. It'd be helpful to know which political affliation each individual twitter account has. Fortunately, I have this data for you! In pol_aff.csv I have included user_ID and their respective political affliation. Use this csv to get the political association of each user account. Note that this file includes user_IDs that don't exist in your corpus of tweets so you'll need to selectively add political affliation based on user IDs. \n",
    "\n",
    "### question 5 [40 points]\n",
    "Perform PCA on individual user accounts using the data in `wc_accounts`. Plot this in 2 dimensions where each data point represents one twitter account. Color code each data point according to account political affliation, with red = republicans and blue = democrats. To make overlapping data points easier to see, reduce the opacity (i.e., the \"alpha\" parameter) to 0.3. \n",
    "\n",
    "You can break this up into several steps:\n",
    "\n",
    "<b>Step 1</b>: Generate a dataframe called `merged_data` with two columns: `user_id` (that comes from `tweets`) and `affliation` (that comes from `pol_aff.csv`). You should have the same number of rows as you have twitter users that you found in question 2. Here's the commands you'll need:\n",
    "1. `read_csv()` (to read in `pol_aff.csv`)\n",
    "2. `unique()` (to get unique twitter users in `tweets` & put them in a new data frame)\n",
    "3. `merge()` (to merge dataframe with `user_id` with dataframe with `affliation`)\n",
    "4. `drop()` (to drop any columns you have in your final dataframe that you don't need)\n",
    "\n",
    "\n",
    "<b>Step 2</b>: \n",
    "<b>Next you want to produce a dataframe called `Graph` in which the rows are twitter users, and the three columns are the twitter user's respective principal component 1 coordinates, principal component 2 coordinates, and affliation.</b> To get the principal components, you'll need to do PCA on `wc_accounts` (from question #4) and then place the results of this PCA into a dataframe that you merge with `affliation` column in step one.\n",
    "\n",
    "<b>Step 3</b>: Once you've constructed the `Graph` dataframe in Step 2, just run the code provided to generate your PCA plot. \n",
    "\n",
    "\n",
    "\n",
    "#### Put your code for question 5 below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for step 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run for Step #3 -- DON'T CHANGE THIS, JUST RUN IT \n",
    "# RUN TO PLOT PCA DATA ONCE YOU HAVE CONSTRUCTED THE GRAPH DATAFRAME\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['democrat', 'republican']\n",
    "colors = ['b','r']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = Graph['affliation'] == target\n",
    "    ax.scatter(Graph.loc[indicesToKeep, 'principal component 1']\n",
    "               , Graph.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50\n",
    "               , alpha =.3)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
